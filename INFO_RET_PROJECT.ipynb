{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOOLEAN MODEL \n",
    "\n",
    "This code provide a simple implementation of a boolean model for information retrieval. \n",
    "\n",
    "\n",
    "It is able to answer to queries in the form of a boolean expression, using the operators AND and OR. The model is able to handle queries with one wildcard (*) which can be in any position of the query.\n",
    "Every terms (both in documents and queries) are normalized and stemmed using PorterStemmer from nltk library.\n",
    "I implemented a k-gram index used for spelling correction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import total_ordering, reduce\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to save the inverted index in the same repository of the code as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_inverted_index(inverted_index, file_path): # save the inverted index in a json file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(inverted_index, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INVERTED INDEX\n",
    "\n",
    "Here is the code for the inverted index. It is a dictionary where the keys are the terms and the values are the list of books in which the term appears. The documents are represented by their id.\n",
    "\n",
    "I tried to remove stopwords to reduce time and space complexity, but in terms of query answering it didn't work well. So I decided to keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INVERTED INDEX\n",
    "def normalize(text):\n",
    "    no_punctuation = re.sub(r'[^\\w^\\s*-]','',text) # remove punctuation\n",
    "    downcase = no_punctuation.lower() # lowercase\n",
    "    return downcase\n",
    "\n",
    "def tokenize(book):\n",
    "    text = normalize(book.description) # normalize books description\n",
    "    return list(text.split()) # return a list of tokens\n",
    "\n",
    "def stemm(book):\n",
    "    ps = PorterStemmer() # stemmer\n",
    "    text = tokenize(book) # tokenize\n",
    "    return list(set([ps.stem(word) for word in text])) \n",
    "\n",
    "    # I tried to remove stopwords but it didn't work well, because it removed too many words and the results were not good, this was the code:\n",
    "    #return list(set([ps.stem(word) for word in text if not word in stopwords])) # return a list of stemmed tokens that are not stopwords\n",
    "\n",
    "# build an inverted index for given documents with tokenization, normalization, stemming and using stopwords\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = {}\n",
    "    print(\"Building inverted index...\")\n",
    "    for doc_id, doc in enumerate(documents): # for each document\n",
    "        for token in stemm(doc): # for each token in the document\n",
    "            if token not in inverted_index: # if the token is not in the inverted index we add it\n",
    "                inverted_index[token] = []\n",
    "            if doc_id not in inverted_index[token]: # if the document is not in the inverted index we add it to the list of documents for that token\n",
    "                inverted_index[token].append(doc_id)    \n",
    "        if (doc_id % 1000 == 0):\n",
    "                print(\"ID: \" + str(doc_id))\n",
    "    return inverted_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOOKS DESCRIPTION\n",
    "\n",
    "Then I created a class to represent the books with title and description as attributes.\n",
    "\n",
    "After that I populated the inverted index with the books in the dataset. \n",
    "\n",
    "First I create the corpus, appending the title and the description of each book. After that I use the corpus to create the inverted index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BOOK CLASS\n",
    "class BooksDescription:\n",
    "    def __init__(self, title, description):\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.title\n",
    "    \n",
    "def books_title():\n",
    "    filename = 'booksummaries.txt' # open file\n",
    "    #with open(filename, 'r') as csv_file: \n",
    "    #    books_name = csv.reader(csv_file, delimiter='\\t')\n",
    "    #    names_table = {}\n",
    "    #    for name in books_name:\n",
    "    #        names_table[name[0]] = name[2] # create a dictionary with book id as key and book title as value\n",
    "           \n",
    "    with open(filename, 'r') as csv_file:\n",
    "        descriptions = csv.reader(csv_file, delimiter='\\t') \n",
    "        corpus = []\n",
    "        # num_lines = sum(1 for _ in descriptions) # count number of lines in the file\n",
    "        csv_file.seek(0)  # reset the reader\n",
    "        for i,desc in enumerate(descriptions):\n",
    "            try: \n",
    "                book = BooksDescription(desc[2], desc[6]) # create a BooksDescription object with title and description\n",
    "                corpus.append(book) # append the object to the corpus\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "            #if i >= num_lines // 2: # uncomment to use only half of the corpus (8000 books instead of 16000)\n",
    "            #   break\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CORPUS and INVERTED INDEX\n",
    "\n",
    "Here I create the inverted list. The code only start if the inverted index is not already created, it checks if the json file is already present in the directory. If it is not present, it creates the inverted index and saves it in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = books_title() # create the corpus\n",
    "try:\n",
    "    inv_index = json.load(open('inverted_index.json')) # load the inverted index from the json file\n",
    "except FileNotFoundError: # if the file is not found\n",
    "    inv_index = build_inverted_index(corpus) # create the inverted index\n",
    "    save_inverted_index(inv_index, 'inverted_index.json') # save the inverted index in a json file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the usage of the inverted index. \n",
    "\n",
    "I print the books id which contains the word 'hobbit' then the titles, searching through the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78, 85, 248, 249, 3130, 6672, 9586, 9933, 15275]\n",
      "The Lord of the Rings\n",
      "The Hobbit\n",
      "The Two Towers\n",
      "The Return of the King\n",
      "The Woods Out Back\n",
      "The Keeper of the Isis Light\n",
      "The Armageddon Rag\n",
      "The Lord of the Rings\n",
      "The Fellowship of the Ring\n"
     ]
    }
   ],
   "source": [
    "print(inv_index['hobbit'])\n",
    "\n",
    "for i in inv_index['hobbit']:\n",
    "    print(corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram inverted index\n",
    "\n",
    "This function creates the n-gram inverted index. It is a dictionary where the keys are the n-grams and the values are the list of terms that contain that n-gram.\n",
    "\n",
    "The n-gram inverted index will be used for spelling correction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_inverted_index(documents, n): # function take in input the documents and the n-gram size\n",
    "    inverted_index = {}\n",
    "    print(\"Building ngram inverted index...\")\n",
    "    for doc_id, doc in enumerate(documents): # for each document\n",
    "        for token in stemm(doc): # for each token in the document\n",
    "            wild_token = \"$\" + token + \"$\" # add initial and final symbol\n",
    "            for i in range(len(wild_token) - n + 1): # for each ngram in the token\n",
    "                ngram = wild_token[i:i+n]  # extract the n-gram\n",
    "                if ngram not in inverted_index:\n",
    "                    inverted_index[ngram] = [] # if the ngram is not in the inverted index we add it\n",
    "                if token not in inverted_index[ngram]:\n",
    "                    inverted_index[ngram].append(token) # if the token is not in the inverted index we add it to the list of tokens for that ngram    \n",
    "        if (doc_id % 1000 == 0):\n",
    "                print(\"ID: \" + str(doc_id))\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = books_title()\n",
    "try:\n",
    "    new_inv_index = json.load(open('ngram_inverted_index.json')) # load the inverted index from the json file\n",
    "except FileNotFoundError:\n",
    "    new_inv_index = build_ngram_inverted_index(corpus, 3) # create the inverted index with 3-grams\n",
    "    save_inverted_index(new_inv_index, 'ngram_inverted_index.json') # save the inverted index in a json file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IR MODEL CLASS\n",
    "\n",
    "Here is the class that represents the IR model. It has the corpus and the two inverted index as attributes. \n",
    "\n",
    "It provides the function to answer to queries with or without wildcard and to correct them if they contain a spelling error.\n",
    "\n",
    "It also checks if the query contain AND or OR operators and returns the intersection of the postings in the first case and the union in the second case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IR MODEL\n",
    "class IR_model:\n",
    "    def __init__(self,corpus, index, ngram_index):\n",
    "        self.corpus = corpus\n",
    "        self._index = index\n",
    "        self._ngram_index = ngram_index\n",
    "        \n",
    "    def answer_query(self, query):\n",
    "        words = query.split()\n",
    "        norm_words = map(normalize, words)\n",
    "        \n",
    "        postings = []\n",
    "        if len(words) == 1 or words[1] == 'and' or words[1] != 'or': # if the query is a single word or a conjunction\n",
    "            if len(words) != 1 and words[1] == 'and': # if the query is a conjunction\n",
    "                words.remove(words[1])  # remove 'and' from the list of words\n",
    "            for word in norm_words:\n",
    "                try:\n",
    "                    res = [self.corpus[i].title for i in self._index[word]] # get the list of books for the word\n",
    "                except KeyError:\n",
    "                    if '*' in word: # if the word contains a wildcard\n",
    "                        res = self.wilcard_query(word) # call the wilcard_query function\n",
    "                    else:\n",
    "                        sub = find_nearest(word, self._ngram_index)   # find the nearest word using jaccard similarity\n",
    "                        print(\"{} not found. Did you mean {}?\".format(word, sub))\n",
    "                        res = [self.corpus[i].title for i in self._index[sub]] # get the list of books for the nearest word\n",
    "\n",
    "                postings.append(res) # append the list of books to the posting list\n",
    "            \n",
    "            if len(set(reduce(np.intersect1d, postings))) == 0: # if the intersection of all postings is empty\n",
    "                print('No results found.')\n",
    "\n",
    "            # return intersection (because we are searching for a boolean query with 'AND' or a query with a single word) of all postings\n",
    "            else:\n",
    "                return set(reduce(np.intersect1d, postings))\n",
    "            \n",
    "        elif words[1] == 'or':\n",
    "            if len(words) != 1 and words[1] == 'or': # if the query is a disjunction\n",
    "                words.remove(words[1])\n",
    "            for word in norm_words:\n",
    "                try:\n",
    "                    res = [self.corpus[i].title for i in self._index[word]]\n",
    "                except KeyError:\n",
    "                    if '*' in word:\n",
    "                        res = self.wilcard_query(word)\n",
    "                    else:\n",
    "                        sub = find_nearest(word, self._ngram_index)\n",
    "                        print(\"{} not found. Did you mean {}?\".format(word, sub))\n",
    "\n",
    "                        res = [self.corpus[i].title for i in self._index[sub]]\n",
    "\n",
    "                postings.append(res)\n",
    "            if len(set(reduce(np.union1d, postings))) == 0: # if the union of all postings is empty\n",
    "                print('No results found.')\n",
    "\n",
    "            # return union of all postings\n",
    "            else:\n",
    "                return set(reduce(np.union1d, postings))\n",
    "        \n",
    "    def wilcard_query(self, word):\n",
    "        sub1 = []\n",
    "        sub2 = []\n",
    "        words = word.split('*') # split the word in two parts\n",
    "        \n",
    "        first_word = words[0]\n",
    "        last_word = words[-1]\n",
    "\n",
    "        if first_word != '': # if the wildcard is not at the beginning of the word\n",
    "            for word in self._index:\n",
    "                if word.startswith(first_word):\n",
    "                    sub1.append(word) # append to sub1 all words that start with the first word\n",
    "\n",
    "        elif first_word == '': # if the wildcard is at the beginning of the word\n",
    "            res = []\n",
    "            for word in self._index:\n",
    "                if word.endswith(last_word): # if the word ends with the last word\n",
    "                    res = res + [self.corpus[i].title for i in self._index[word]] # append to res the list of books that finish with last word\n",
    "            \n",
    "        if last_word != '': # if the wildcard is not at the end of the word\n",
    "            for word in self._index:\n",
    "                if word.endswith(last_word):\n",
    "                    sub2.append(word) # append to sub 2 all words that end with the last word\n",
    "\n",
    "        elif last_word == '': # if the wildcard is at the end of the word\n",
    "            res = []\n",
    "            for word in self._index:\n",
    "                if word.startswith(first_word):\n",
    "                    res = res + [self.corpus[i].title for i in self._index[word]]\n",
    "\n",
    "        if first_word != '' and last_word != '': # if the wildcard is in the middle of the word\n",
    "            sub = list([sub1] + [sub2]) # concatenate sub1 and sub2\n",
    "            ss = set(reduce(np.intersect1d, sub)) # get the intersection of sub1 and sub2\n",
    "            res = []\n",
    "            for s in ss:\n",
    "                res = res + [self.corpus[i].title for i in self._index[s]] # append to res the list of books for each word in the intersection\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPELLING CORRECTION\n",
    "\n",
    "Here is the code for the spelling correction. It takes as input a word and the k-gram inverted index. Provide a list of words that contains at least one of the n-grams of the input word. \n",
    "\n",
    "Then for all of these words it calculates the Jaccard similarity with the input word. Jaccard similarity is the intersection of the n-grams of the input word and the n-grams of the word in the list, divided by the union.\n",
    "\n",
    "Then it returns the word with the highest Jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPELLING CORRECTION using Jaccard similarity\n",
    "def ngrams(word, n):\n",
    "    return [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "\n",
    "def find_nearest(word, index):    \n",
    "    # Get the list of k-grams for the input word\n",
    "    word_ngrams = ngrams(\"$\" + word + \"$\", 3)\n",
    "    # Build a set of all words that have any of these k-grams\n",
    "    words_with_kgrams = set()\n",
    "    for ngram in word_ngrams:\n",
    "        try: # check if ngram is in inverted index, if not, pass\n",
    "            words_with_kgrams.update(index[ngram]) \n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    # Compute the Jaccard similarity coefficient for each candidate word,\n",
    "    # and take the one that maximizes it\n",
    "    scores = []\n",
    "    for w in words_with_kgrams: # for each word in the set of words with k-grams\n",
    "        w_ngrams = ngrams(\"$\" + w + \"$\", 3) # get the list of k-grams for the word\n",
    "        scores.append((w, len(set(word_ngrams).intersection(w_ngrams)) / len(set(word_ngrams).union(w_ngrams)))) # compute the Jaccard similarity coefficient and append it to the list of scores  \n",
    "    return max(scores, key=lambda x: x[1])[0] # return the word with the highest Jaccard similarity coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obbit not found. Did you mean hobbit?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'The Fellowship of the Ring',\n",
       " 'The Hobbit',\n",
       " 'The Lord of the Rings',\n",
       " 'The Return of the King',\n",
       " 'The Two Towers'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir = IR_model(corpus, inv_index, new_inv_index)\n",
    "ir.answer_query(\"obbit gollum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First query: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Harry Potter and the Chamber of Secrets',\n",
       " 'Harry Potter and the Goblet of Fire',\n",
       " 'Harry Potter and the Half-Blood Prince',\n",
       " 'Harry Potter and the Order of the Phoenix',\n",
       " 'Harry Potter and the Prisoner of Azkaban'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir = IR_model(corpus, inv_index, new_inv_index)\n",
    "print('First query: ')\n",
    "ir.answer_query(\"potter and voldemort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second query: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Fantastic Beasts and Where to Find Them',\n",
       " 'Harry Potter and the Chamber of Secrets'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Second query: ')\n",
    "ir.answer_query(\"azkaban\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third query: \n",
      "harry not found. Did you mean harri?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Fantastic Beasts and Where to Find Them',\n",
       " 'Harry Potter and the Chamber of Secrets',\n",
       " 'Harry Potter and the Goblet of Fire',\n",
       " 'Harry Potter and the Half-Blood Prince',\n",
       " 'Harry Potter and the Order of the Phoenix',\n",
       " 'Harry Potter and the Prisoner of Azkaban',\n",
       " 'Operation Chaos',\n",
       " 'Point Blanc',\n",
       " 'Quidditch Through the Ages',\n",
       " 'Wolves of the Calla'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Third query: ')\n",
    "ir.answer_query(\"harry not potter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourth query: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Gradisil',\n",
       " 'Permutation City',\n",
       " 'The Armageddon Rag',\n",
       " 'The Fellowship of the Ring',\n",
       " 'The Hobbit',\n",
       " 'The Keeper of the Isis Light',\n",
       " 'The Lord of the Rings',\n",
       " 'The Return of the King',\n",
       " 'The Two Towers',\n",
       " 'The Woods Out Back'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Fourth query:')\n",
    "ir.answer_query(\"hobb*t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fifth query: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'99 Coffins',\n",
       " 'A Beautiful Blue Death',\n",
       " 'A Kestrel for a Knave',\n",
       " 'American Born Chinese',\n",
       " 'An Enquiry Concerning the Principles of Morals',\n",
       " 'Arabian Jazz',\n",
       " \"Assassin's Quest\",\n",
       " 'Avatar',\n",
       " 'Beatles',\n",
       " 'Billy the Kid',\n",
       " 'Boy Meets Boy',\n",
       " 'China Marine',\n",
       " \"Darwin's Dangerous Idea\",\n",
       " 'Dexter by Design',\n",
       " 'Dexter is Delicious',\n",
       " 'Dragon Age: The Stolen Throne',\n",
       " 'Duma Key',\n",
       " 'Farewell to Manzanar',\n",
       " \"Friday's Child\",\n",
       " 'Fungus the Bogeyman',\n",
       " 'Go',\n",
       " 'Goliath',\n",
       " 'Gradisil',\n",
       " 'Hallam Foe',\n",
       " 'Harlequin',\n",
       " 'Hatyapuri',\n",
       " 'House Rules',\n",
       " 'I Am David',\n",
       " 'In a Dry Season',\n",
       " 'Incidents in the Life of a Slave Girl',\n",
       " 'Isle of the Dead',\n",
       " \"King Solomon's Carpet\",\n",
       " 'Little Lord Fauntleroy',\n",
       " 'Living Dead Girl',\n",
       " 'London Fields',\n",
       " 'Michael Vey: The Prisoner of Cell 25',\n",
       " 'Mystery of the Whale Tattoo',\n",
       " 'No Highway',\n",
       " 'Our Southern Highlanders',\n",
       " 'Pacific Vortex!',\n",
       " 'Palace Walk',\n",
       " 'Pattern Recognition',\n",
       " 'Permutation City',\n",
       " \"Pudd'nhead Wilson\",\n",
       " 'Revenge in the Silent Tomb',\n",
       " 'Sasquatch',\n",
       " 'Six Records of a Floating Life',\n",
       " 'Starfighters of Adumar',\n",
       " 'Summer and the City',\n",
       " 'The Also People',\n",
       " 'The Armageddon Rag',\n",
       " 'The Black Dwarf',\n",
       " 'The Canary Trainer',\n",
       " 'The Changeover: A Supernatural Romance',\n",
       " 'The Children of Húrin',\n",
       " 'The Color Purple',\n",
       " 'The Dead Zone',\n",
       " 'The Deptford Mice Almanack',\n",
       " 'The Documents in the Case',\n",
       " 'The Door in the Wall',\n",
       " 'The Fellowship of the Ring',\n",
       " 'The History of Love',\n",
       " 'The Hobbit',\n",
       " 'The Judas Window',\n",
       " 'The Keeper of the Isis Light',\n",
       " 'The Leopard',\n",
       " 'The Lord of the Rings',\n",
       " 'The Magician',\n",
       " 'The Man Who Laughs',\n",
       " 'The Natural',\n",
       " 'The Oaken Throne',\n",
       " 'The Pilgrim of Hate',\n",
       " 'The Report Card',\n",
       " 'The Return of the King',\n",
       " 'The Second Invasion from Mars',\n",
       " 'The Spanish Gardener',\n",
       " 'The Star Beast',\n",
       " 'The Stardroppers',\n",
       " 'The Thorn Birds',\n",
       " 'The Threateners',\n",
       " 'The Two Towers',\n",
       " 'The Whispering Road',\n",
       " 'The Witch of Edmonton',\n",
       " 'The Woods Out Back',\n",
       " 'The Years',\n",
       " 'Undone',\n",
       " \"Who's Your City?\",\n",
       " 'Whose Body?',\n",
       " \"You Can't Take It with You\",\n",
       " 'Your Heart Belongs to Me',\n",
       " 'iWoz: Computer Geek to Cult Icon - How I Invented the Personal Computer, Co-Founded Apple, and Had Fun Doing It'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Fifth query: ')\n",
    "ir.answer_query(\"hobb*\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
