{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information retrieval project\n",
    "\n",
    "Goal is to create an IR model that can perform both boolean (AND, OR and NOT), wildcard and phrase queries.\n",
    "\n",
    "Perform normalization and stemming.\n",
    "\n",
    "Spelling correction.\n",
    "\n",
    "Evaluate the system on test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents():\n",
    "    f = open(\"archive/CISI.ALL\")\n",
    "    merged = \" \"\n",
    "    i = 0\n",
    "    for a_line in f.readlines ():\n",
    "            if a_line.startswith (\".\"):\n",
    "                i += 1\n",
    "                merged += \"\\n\" + a_line.strip ()\n",
    "            else:\n",
    "                i += 1\n",
    "                merged += \" \" + a_line.strip ()\n",
    "        # updates the merged variable using a for-loop\n",
    "    documents = {}\n",
    "    content = \"\"\n",
    "    doc_id = \"\"\n",
    "    # each entry in the dictioanry contains key = doc_id and value = content\n",
    "\n",
    "    for line in merged.split (\"\\n\"):\n",
    "        #print(a_line)\n",
    "        if line.startswith (\".I\"):\n",
    "            doc_id = line.split (\" \") [1].strip()\n",
    "        elif line.startswith (\".X\"):\n",
    "            documents[doc_id] = content\n",
    "            content = \"\"\n",
    "            doc_id = \"\"\n",
    "        else:\n",
    "            content += line.strip ()[3:] + \" \"\n",
    "            #Extract after . a letter and a space\n",
    "    f.close ()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries():\n",
    "    f = open(\"archive/CISI.QRY\")\n",
    "\n",
    "    merged = \"\"\n",
    "\n",
    "    for line in f.readlines ():\n",
    "        if line.startswith (\".\"):\n",
    "            merged += \"\\n\" + line.strip ()\n",
    "        else:\n",
    "            merged += \" \" + line.strip ()\n",
    "    \n",
    "    queries = {}\n",
    "\n",
    "    content = \"\"\n",
    "    qry_id = \"\"\n",
    "\n",
    "    for line in merged.split (\"\\n\"):\n",
    "        if line.startswith(\".I\"):\n",
    "            if not content == \"\":\n",
    "                queries [qry_id] = content\n",
    "                content = \"\"\n",
    "                qry_id = \"\"\n",
    "            # add an enrty to the dictionary when you encounter an .I identifier\n",
    "            qry_id = line.split(\" \")[1].strip ()\n",
    "        # otherwise, keep adding content to the content variable\n",
    "        elif line.startswith (\".W\") or line.startswith (\".T\"):\n",
    "            content += line.strip ()[3:] + \" \"\n",
    "    queries [qry_id] = content\n",
    "    f.close ()\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_relevance():\n",
    "    f = open(\"archive/CISI.REL\")\n",
    "    mappings = {}\n",
    "    \n",
    "    for line in f.readlines ():\n",
    "        voc = line.strip ().split ()\n",
    "        key = voc[0].strip ()\n",
    "        current_value = voc[1].strip()\n",
    "        value = []\n",
    "        # update the entry in the mappings dictionary with the current value\n",
    "        if key in mappings.keys ():\n",
    "            value = mappings.get (key)\n",
    "        value.append (current_value)\n",
    "        mappings [key] = value\n",
    "    f.close ()\n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n",
      " 18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. \n"
     ]
    }
   ],
   "source": [
    "documents = read_documents()\n",
    "print(len(documents))\n",
    "print(documents[\"1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles? \n"
     ]
    }
   ],
   "source": [
    "queries = read_queries()\n",
    "print(len(queries))\n",
    "print(queries[\"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "['28', '35', '38', '42', '43', '52', '65', '76', '86', '150', '189', '192', '193', '195', '215', '269', '291', '320', '429', '465', '466', '482', '483', '510', '524', '541', '576', '582', '589', '603', '650', '680', '711', '722', '726', '783', '813', '820', '868', '869', '894', '1162', '1164', '1195', '1196', '1281']\n"
     ]
    }
   ],
   "source": [
    "relevance = read_relevance()\n",
    "print(len(relevance))\n",
    "print(relevance[\"1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(text):\n",
    "    terms = {}\n",
    "    ps = PorterStemmer()\n",
    "    word_list = [ps.stem(word) for word in word_tokenize(text.lower()) if not word in string.punctuation]\n",
    "    #print(word_list)\n",
    "    for word in word_list:\n",
    "        terms[word] = terms.get(word, 0) + 1\n",
    "    return terms\n",
    "\n",
    "doc_terms = {}\n",
    "qry_terms = {}\n",
    "for doc_id in documents.keys ():\n",
    "    text = documents.get (doc_id)\n",
    "    #print(word_tokenize(text.lower()))\n",
    "    doc_terms[doc_id] = get_terms(documents.get(doc_id))\n",
    "\n",
    "for qry_id in queries.keys ():\n",
    "    # populate the term frequency dictionaries for all documents and all queries\n",
    "    qry_terms [qry_id] = get_terms (queries.get (qry_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n",
      "{'18': 1, 'edit': 4, 'of': 7, 'the': 10, 'dewey': 3, 'decim': 2, 'classif': 2, 'comaromi': 1, 'j.p.': 1, 'present': 1, 'studi': 1, 'is': 2, 'a': 2, 'histori': 2, 'first': 2, 'ddc': 2, 'wa': 1, 'publish': 1, 'in': 4, '1876': 1, 'eighteenth': 1, '1971': 1, 'and': 3, 'futur': 1, 'will': 1, 'continu': 1, 'to': 2, 'appear': 1, 'as': 1, 'need': 1, 'spite': 1, \"'s\": 1, 'long': 1, 'healthi': 1, 'life': 1, 'howev': 1, 'it': 1, 'full': 1, 'stori': 1, 'ha': 2, 'never': 1, 'been': 2, 'told': 1, 'there': 1, 'have': 1, 'biographi': 1, 'that': 2, 'briefli': 1, 'describ': 1, 'hi': 1, 'system': 1, 'but': 1, 'thi': 2, 'attempt': 1, 'provid': 1, 'detail': 1, 'work': 1, 'more': 1, 'than': 1, 'ani': 1, 'other': 1, 'spur': 1, 'growth': 1, 'librarianship': 1, 'countri': 1, 'abroad': 1}\n",
      "66\n",
      "112\n",
      "{'what': 3, 'problem': 1, 'and': 1, 'concern': 1, 'are': 2, 'there': 1, 'in': 2, 'make': 1, 'up': 1, 'descript': 1, 'titl': 3, 'difficulti': 1, 'involv': 1, 'automat': 1, 'retriev': 1, 'articl': 2, 'from': 1, 'approxim': 1, 'is': 1, 'the': 2, 'usual': 1, 'relev': 1, 'of': 2, 'content': 1, 'to': 1, 'their': 1}\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print (len (doc_terms))\n",
    "print (doc_terms.get (\"1\"))\n",
    "print (len (doc_terms.get(\"1\")))\n",
    "print (len (qry_terms))\n",
    "print (qry_terms.get(\"1\"))\n",
    "print (len (qry_terms.get(\"1\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10526315789473684\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    query_set = set(query)\n",
    "    doc_set = set(document)\n",
    "    return len(query_set & doc_set) / len(query_set | doc_set)\n",
    "\n",
    "\n",
    "print(jaccard_similarity(qry_terms.get(\"1\"), doc_terms.get(\"35\")))  # Output vicino a 1 indica alta rilevanza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08235294117647059\n",
      "0.1\n",
      "0.08536585365853659\n",
      "0.05747126436781609\n",
      "0.06299212598425197\n",
      "0.059602649006622516\n",
      "0.08641975308641975\n",
      "0.07586206896551724\n",
      "0.05785123966942149\n",
      "0.07407407407407407\n",
      "0.10975609756097561\n",
      "0.06153846153846154\n",
      "0.07228915662650602\n",
      "0.06722689075630252\n",
      "0.08108108108108109\n",
      "0.06832298136645963\n",
      "0.05439330543933055\n",
      "0.0761904761904762\n",
      "0.051470588235294115\n",
      "0.06363636363636363\n",
      "0.057971014492753624\n",
      "0.07317073170731707\n",
      "0.08333333333333333\n",
      "0.1388888888888889\n",
      "0.08163265306122448\n",
      "0.10975609756097561\n",
      "0.0425531914893617\n",
      "0.08609271523178808\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 30):\n",
    "    print(jaccard_similarity(qry_terms.get(\"1\"), doc_terms.get(str(i))))  # Output vicino a 1 indica alta rilevanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INVERTED INDEX\n",
    "def normalize(text):\n",
    "    no_punctuation = re.sub(r'[^\\w^\\s*-]','',text) # remove punctuation\n",
    "    downcase = no_punctuation.lower() # lowercase\n",
    "    return downcase\n",
    "\n",
    "def tokenize(content):\n",
    "    text = normalize(content)\n",
    "    return list(text.split()) # return a list of tokens\n",
    "\n",
    "def Lstemm(content):\n",
    "    ps = LancasterStemmer() # stemmer\n",
    "    text = tokenize(content) # tokenize\n",
    "    return list(set([ps.stem(word) for word in text]))\n",
    "def Pstemm(content):\n",
    "    ps = PorterStemmer() # stemmer\n",
    "    text = tokenize(content) # tokenize\n",
    "    return list(set([ps.stem(word) for word in text])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inverted index\n",
    "def create_inverted_index_no_norm(documents):\n",
    "    inverted_index = {}\n",
    "    for doc_id, content in documents.items():\n",
    "        for token in content.split():\n",
    "            if token in inverted_index.keys():\n",
    "                if doc_id not in inverted_index[token]:\n",
    "                    inverted_index[token].append(doc_id)\n",
    "            else:\n",
    "                inverted_index[token] = [doc_id]\n",
    "        #if (int(doc_id) % 100 == 0):\n",
    "        #    print(\"ID: \" + str(doc_id))\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inverted index\n",
    "def create_inverted_index_P(documents):\n",
    "    inverted_index = {}\n",
    "    for doc_id, content in documents.items():\n",
    "        #print(content)\n",
    "        for token in Pstemm(content):\n",
    "            if token in inverted_index.keys():\n",
    "                if doc_id not in inverted_index[token]:\n",
    "                    inverted_index[token].append(doc_id)\n",
    "            else:\n",
    "                inverted_index[token] = [doc_id]\n",
    "        #if (int(doc_id) % 100 == 0):\n",
    "        #    print(\"ID: \" + str(doc_id))\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inverted index\n",
    "def create_inverted_index_L(documents):\n",
    "    inverted_index = {}\n",
    "    for doc_id, content in documents.items():\n",
    "        #print(content)\n",
    "        for token in Lstemm(content):\n",
    "            if token in inverted_index.keys():\n",
    "                if doc_id not in inverted_index[token]:\n",
    "                    inverted_index[token].append(doc_id)\n",
    "            else:\n",
    "                inverted_index[token] = [doc_id]\n",
    "        #if (int(doc_id) % 100 == 0):\n",
    "        #    print(\"ID: \" + str(doc_id))\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len no norm: 21967\n",
      "Len L: 7556\n",
      "Len P: 8554\n"
     ]
    }
   ],
   "source": [
    "inv_index_no_norm = create_inverted_index_no_norm(documents)\n",
    "\n",
    "inv_index_L = create_inverted_index_L(documents)\n",
    "\n",
    "inv_index_P = create_inverted_index_P(documents)\n",
    "\n",
    "print(f\"Len no norm: {len(inv_index_no_norm)}\")\n",
    "print(f\"Len L: {len(inv_index_L)}\")\n",
    "print(f\"Len P: {len(inv_index_P)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_inverted_index(inverted_index):\n",
    "    ordered_inverted_index = {}\n",
    "    for key in sorted(inverted_index.keys()):\n",
    "        ordered_inverted_index[key] = inverted_index[key]\n",
    "    return ordered_inverted_index\n",
    "\n",
    "ordered = order_inverted_index(inv_index_no_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5',\n",
       " '16',\n",
       " '42',\n",
       " '176',\n",
       " '233',\n",
       " '275',\n",
       " '282',\n",
       " '290',\n",
       " '328',\n",
       " '341',\n",
       " '345',\n",
       " '363',\n",
       " '379',\n",
       " '404',\n",
       " '405',\n",
       " '417',\n",
       " '428',\n",
       " '455',\n",
       " '476',\n",
       " '478',\n",
       " '479',\n",
       " '486',\n",
       " '559',\n",
       " '577',\n",
       " '610',\n",
       " '669',\n",
       " '694',\n",
       " '701',\n",
       " '722',\n",
       " '769',\n",
       " '791',\n",
       " '797',\n",
       " '798',\n",
       " '838',\n",
       " '857',\n",
       " '862',\n",
       " '945',\n",
       " '954',\n",
       " '958',\n",
       " '1029',\n",
       " '1075',\n",
       " '1180',\n",
       " '1204',\n",
       " '1217',\n",
       " '1237',\n",
       " '1380',\n",
       " '1395',\n",
       " '1398',\n",
       " '1415',\n",
       " '1423']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_index_P.get(\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: apple AND (banana OR cherry)\n",
      "Matching Documents: {1, 2}\n",
      "\n",
      "Query: apple OR banana\n",
      "Matching Documents: {1, 2, 3, 4}\n",
      "\n",
      "Query: apple AND NOT banana\n",
      "Matching Documents: {1, 4}\n",
      "\n",
      "Query: NOT cherry\n",
      "Matching Documents: {2, 4, 6}\n",
      "\n",
      "Query: (apple OR banana) AND (cherry OR date)\n",
      "Matching Documents: {1, 3}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_query(query):\n",
    "    \"\"\"\n",
    "    Splits the query into tokens (terms, operators, parentheses).\n",
    "    Operators are normalized to uppercase and terms to lowercase.\n",
    "    \"\"\"\n",
    "    # This regex captures words, parentheses, and the Boolean operators.\n",
    "    tokens = re.findall(r'\\(|\\)|\\bAND\\b|\\bOR\\b|\\bNOT\\b|\\w+', query, flags=re.IGNORECASE)\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.upper() in {\"AND\", \"OR\", \"NOT\"}:\n",
    "            normalized_tokens.append(token.upper())\n",
    "        elif token in {\"(\", \")\"}:\n",
    "            normalized_tokens.append(token)\n",
    "        else:\n",
    "            normalized_tokens.append(token.lower())\n",
    "    return normalized_tokens\n",
    "\n",
    "def shunting_yard(tokens):\n",
    "    \"\"\"\n",
    "    Converts infix expression tokens to postfix (RPN) using the Shunting-yard algorithm.\n",
    "    Operator precedence: NOT > AND > OR.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    op_stack = []\n",
    "    precedence = {\"NOT\": 3, \"AND\": 2, \"OR\": 1}\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Check if token is an operator first.\n",
    "        if token in {\"AND\", \"OR\", \"NOT\"}:\n",
    "            # Pop operators with higher or equal precedence from the op_stack.\n",
    "            while (op_stack and op_stack[-1] != \"(\" and \n",
    "                   op_stack[-1] in precedence and \n",
    "                   precedence[op_stack[-1]] >= precedence[token]):\n",
    "                output.append(op_stack.pop())\n",
    "            op_stack.append(token)\n",
    "        elif token == \"(\":\n",
    "            op_stack.append(token)\n",
    "        elif token == \")\":\n",
    "            # Pop until an '(' is encountered.\n",
    "            while op_stack and op_stack[-1] != \"(\":\n",
    "                output.append(op_stack.pop())\n",
    "            if op_stack and op_stack[-1] == \"(\":\n",
    "                op_stack.pop()  # Remove the '('\n",
    "            else:\n",
    "                raise ValueError(\"Mismatched parentheses in query.\")\n",
    "        else:\n",
    "            # The token is a term.\n",
    "            output.append(token)\n",
    "    \n",
    "    # Append any remaining operators.\n",
    "    while op_stack:\n",
    "        op = op_stack.pop()\n",
    "        if op in {\"(\", \")\"}:\n",
    "            raise ValueError(\"Mismatched parentheses in query.\")\n",
    "        output.append(op)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def evaluate_postfix(postfix_tokens, inverted_index, universal_set):\n",
    "    \"\"\"\n",
    "    Evaluates the Boolean query given in postfix notation.\n",
    "    Returns a set of document IDs matching the query.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for token in postfix_tokens:\n",
    "        if token in {\"AND\", \"OR\", \"NOT\"}:\n",
    "            if token == \"NOT\":\n",
    "                if not stack:\n",
    "                    raise ValueError(\"Insufficient operands for NOT operator.\")\n",
    "                operand = stack.pop()\n",
    "                result = universal_set - operand\n",
    "                stack.append(result)\n",
    "            else:\n",
    "                if len(stack) < 2:\n",
    "                    raise ValueError(f\"Insufficient operands for {token} operator.\")\n",
    "                right = stack.pop()\n",
    "                left = stack.pop()\n",
    "                if token == \"AND\":\n",
    "                    result = left & right  # Intersection\n",
    "                elif token == \"OR\":\n",
    "                    result = left | right  # Union\n",
    "                stack.append(result)\n",
    "        else:\n",
    "            # The token is a term.\n",
    "            posting = inverted_index.get(token, set())\n",
    "            stack.append(posting)\n",
    "    \n",
    "    if len(stack) != 1:\n",
    "        raise ValueError(f\"Error in evaluation: stack should have exactly one element, but got {stack}\")\n",
    "    \n",
    "    return stack[0]\n",
    "\n",
    "def evaluate_boolean_query(query, inverted_index, universal_set):\n",
    "    \"\"\"\n",
    "    Processes a Boolean query:\n",
    "      1. Tokenizes the query.\n",
    "      2. Converts it to postfix notation.\n",
    "      3. Evaluates the postfix expression.\n",
    "    Returns a set of document IDs that satisfy the query.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_query(query)\n",
    "    # Debug: Uncomment to see tokens\n",
    "    # print(\"Tokens:\", tokens)\n",
    "    \n",
    "    postfix = shunting_yard(tokens)\n",
    "    # Debug: Uncomment to see postfix notation\n",
    "    # print(\"Postfix:\", postfix)\n",
    "    \n",
    "    result = evaluate_postfix(postfix, inverted_index, universal_set)\n",
    "    return result\n",
    "\n",
    "# ------------------------------\n",
    "# Example Usage\n",
    "# ------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample inverted index: term -> set of document IDs.\n",
    "    inverted_index = {\n",
    "        \"apple\": {1, 2, 4},\n",
    "        \"banana\": {2, 3},\n",
    "        \"cherry\": {1, 3, 5},\n",
    "        \"date\": {3, 6}\n",
    "    }\n",
    "\n",
    "    # Universal set: all document IDs.\n",
    "    universal_set = {1, 2, 3, 4, 5, 6}\n",
    "\n",
    "    # List of example Boolean queries.\n",
    "    queries = [\n",
    "        \"apple AND (banana OR cherry)\",   # Expected: Intersection of {1,2,4} and union of {2,3} and {1,3,5} -> {1,2}\n",
    "        \"apple OR banana\",                  # Expected union: {1,2,3,4}\n",
    "        \"apple AND NOT banana\",             # Expected: {1,2,4} - {2,3} -> {1,4}\n",
    "        \"NOT cherry\",                       # Expected: universal_set - {1,3,5} -> {2,4,6}\n",
    "        \"(apple OR banana) AND (cherry OR date)\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        try:\n",
    "            result = evaluate_boolean_query(query, inverted_index, universal_set)\n",
    "            print(f\"Query: {query}\\nMatching Documents: {result}\\n\")\n",
    "        except ValueError as ve:\n",
    "            print(f\"Query: {query}\\nError: {ve}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: NOT class AND NOT game\n",
      "Matching Documents: {1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 283, 284, 285, 286, 287, 288, 289, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 342, 343, 344, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 477, 480, 481, 482, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 695, 696, 697, 698, 699, 700, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 792, 793, 794, 795, 796, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 858, 859, 860, 861, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 946, 947, 948, 949, 950, 951, 952, 953, 955, 956, 957, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1396, 1397, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize_query(query):\n",
    "    \"\"\"\n",
    "    Splits the query into tokens (terms, operators, and parentheses).\n",
    "    Operators are normalized to uppercase.\n",
    "    Terms are lowercased and stemmed using the Porter Stemmer.\n",
    "    \"\"\"\n",
    "    # This regex captures words, parentheses, and Boolean operators.\n",
    "    tokens = re.findall(r'\\(|\\)|\\bAND\\b|\\bOR\\b|\\bNOT\\b|\\w+', query, flags=re.IGNORECASE)\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        # Check if token is an operator\n",
    "        if token.upper() in {\"AND\", \"OR\", \"NOT\"}:\n",
    "            normalized_tokens.append(token.upper())\n",
    "        elif token in {\"(\", \")\"}:\n",
    "            normalized_tokens.append(token)\n",
    "        else:\n",
    "            # For terms, lowercase and apply stemming\n",
    "            normalized_tokens.append(stemmer.stem(token.lower()))\n",
    "    return normalized_tokens\n",
    "\n",
    "def shunting_yard(tokens):\n",
    "    \"\"\"\n",
    "    Converts infix expression tokens to postfix (RPN) using the Shunting-yard algorithm.\n",
    "    Operator precedence: NOT > AND > OR.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    op_stack = []\n",
    "    precedence = {\"NOT\": 3, \"AND\": 2, \"OR\": 1}\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in {\"AND\", \"OR\", \"NOT\"}:\n",
    "            # Pop operators with higher or equal precedence\n",
    "            while (op_stack and op_stack[-1] != \"(\" and \n",
    "                   op_stack[-1] in precedence and \n",
    "                   precedence[op_stack[-1]] >= precedence[token]):\n",
    "                output.append(op_stack.pop())\n",
    "            op_stack.append(token)\n",
    "        elif token == \"(\":\n",
    "            op_stack.append(token)\n",
    "        elif token == \")\":\n",
    "            # Pop until an '(' is encountered\n",
    "            while op_stack and op_stack[-1] != \"(\":\n",
    "                output.append(op_stack.pop())\n",
    "            if op_stack and op_stack[-1] == \"(\":\n",
    "                op_stack.pop()  # Remove the '('\n",
    "            else:\n",
    "                raise ValueError(\"Mismatched parentheses in query.\")\n",
    "        else:\n",
    "            output.append(token)\n",
    "    \n",
    "    while op_stack:\n",
    "        op = op_stack.pop()\n",
    "        if op in {\"(\", \")\"}:\n",
    "            raise ValueError(\"Mismatched parentheses in query.\")\n",
    "        output.append(op)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def evaluate_postfix(postfix_tokens, inverted_index, universal_set):\n",
    "    \"\"\"\n",
    "    Evaluates the Boolean query given in postfix notation.\n",
    "    Returns a set of document IDs matching the query.\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    for token in postfix_tokens:\n",
    "        if token in {\"AND\", \"OR\", \"NOT\"}:\n",
    "            if token == \"NOT\":\n",
    "                if not stack:\n",
    "                    raise ValueError(\"Insufficient operands for NOT operator.\")\n",
    "                operand = stack.pop()\n",
    "                result = universal_set - operand\n",
    "                stack.append(result)\n",
    "            else:\n",
    "                if len(stack) < 2:\n",
    "                    raise ValueError(f\"Insufficient operands for {token} operator.\")\n",
    "                right = stack.pop()\n",
    "                left = stack.pop()\n",
    "                if token == \"AND\":\n",
    "                    result = left & right  # Intersection\n",
    "                elif token == \"OR\":\n",
    "                    result = left | right  # Union\n",
    "                stack.append(result)\n",
    "        else:\n",
    "            posting = inverted_index.get(token, set())\n",
    "            stack.append(posting)\n",
    "    \n",
    "    if len(stack) != 1:\n",
    "        raise ValueError(f\"Error in evaluation: stack should have exactly one element, but got {stack}\")\n",
    "    \n",
    "    return stack[0]\n",
    "\n",
    "def evaluate_boolean_query(query, inverted_index, universal_set):\n",
    "    \"\"\"\n",
    "    Processes a Boolean query:\n",
    "      1. Tokenizes the query (with stemming).\n",
    "      2. Converts it to postfix notation.\n",
    "      3. Evaluates the postfix expression.\n",
    "    Returns a set of document IDs that satisfy the query.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_query(query)\n",
    "    postfix = shunting_yard(tokens)\n",
    "    result = evaluate_postfix(postfix, inverted_index, universal_set)\n",
    "    return result \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    universal_set = set(range(1, 1461))\n",
    "\n",
    "\n",
    "    queries = [\n",
    "        \"NOT class AND NOT game\"\n",
    "    ]\n",
    "    converted_index = {term: set(map(int, doc_ids)) for term, doc_ids in inv_index_P.items()}\n",
    "\n",
    "    for query in queries:\n",
    "        try:\n",
    "            result = evaluate_boolean_query(query, converted_index, universal_set)\n",
    "            print(f\"Query: {query}\\nMatching Documents: {result}\\n\")\n",
    "        except ValueError as ve:\n",
    "            print(f\"Query: {query}\\nError: {ve}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
